{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1PeT7zQXWhoqP8Qu0NKGM_dl_ATy9Vwl9",
      "authorship_tag": "ABX9TyOHoNSvaGZH+l/M3IJArYtm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahidulislamkhokon/Python/blob/master/tweet_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessaries libraries"
      ],
      "metadata": {
        "id": "4wZqArtPYnKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "AOjdGG-2ZA-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.4.4"
      ],
      "metadata": {
        "id": "qPbAI0laP20g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKVH5ekVYcyN"
      },
      "outputs": [],
      "source": [
        "# # Importing important libraris\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# # text processing libraries\n",
        "# import re\n",
        "# import string\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# # for plotting graphs and visualization\n",
        "# from plotly.offline import iplot\n",
        "# import matplotlib.pyplot as plt\n",
        "# import plotly.graph_objs as go\n",
        "# import cufflinks\n",
        "# cufflinks.go_offline()\n",
        "# cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
        "# import seaborn as sns\n",
        "\n",
        "\n",
        "# # File system manangement\n",
        "# import os\n",
        "\n",
        "# from wordcloud import WordCloud\n",
        "\n",
        "# # sklearn \n",
        "# from sklearn import model_selection\n",
        "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# # Pytorch\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from tqdm import tqdm\n",
        "# import string\n",
        "\n",
        "# #Transformers\n",
        "# import transformers\n",
        "# import tokenizers\n",
        "# from transformers import BertTokenizer, BertModel\n",
        "# from transformers import AdamW\n",
        "# from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# # Suppress warnings \n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np \n",
        "import random\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from plotly import graph_objs as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "from collections import Counter\n",
        "\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import nltk\n",
        "import spacy\n",
        "import random\n",
        "from spacy.util import compounding\n",
        "from spacy.util import minibatch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "\n"
      ],
      "metadata": {
        "id": "E_EGGjD3xlQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for using different color\n",
        "def random_colours(number_of_colors):\n",
        "    '''\n",
        "    Simple function for random colours generation.\n",
        "    Input:\n",
        "        number_of_colors - integer value indicating the number of colours which are going to be generated.\n",
        "    Output:\n",
        "        Color in the following format: ['#E86DA4'] .\n",
        "    '''\n",
        "    colors = []\n",
        "    for i in range(number_of_colors):\n",
        "        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n",
        "    return colors"
      ],
      "metadata": {
        "id": "HMYAVdGR_xQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "jeEB1dHvah34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/test.csv\")\n",
        "#ss = pd.read_csv(\"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/test.csv\")\n",
        "ss = pd.read_csv(\"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/submission.csv\")\n",
        "\n",
        "print(f\"Training data shape: {train.shape}\")\n",
        "print(f\"Test data shape: {test.shape}\")"
      ],
      "metadata": {
        "id": "mubwZoC2AHEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "iZOtllU4A616"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "sxgEDj2VjPOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.info()"
      ],
      "metadata": {
        "id": "fKYDLik2jYao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the first few rows\n",
        "train.head()"
      ],
      "metadata": {
        "id": "9geHsi0Ue8yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.describe()"
      ],
      "metadata": {
        "id": "d_xtWfg-jfp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets look at the distribution of tweets in the train set"
      ],
      "metadata": {
        "id": "uhkfCNc4jvw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\n",
        "temp.style.background_gradient(cmap='Purples')"
      ],
      "metadata": {
        "id": "uvvLpKjBjwhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='sentiment',data=train)"
      ],
      "metadata": {
        "id": "YgJTkdbRkoLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's draw a Funnel-Chart for better visualization"
      ],
      "metadata": {
        "id": "Vu96x3BHm9Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure(go.Funnelarea(\n",
        "    text =temp.sentiment,\n",
        "    values = temp.text,\n",
        "    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n",
        "    ))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "IdazFjmunIMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating Meta-Features"
      ],
      "metadata": {
        "id": "o-IPSYOUuLoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "metadata": {
        "id": "Q5GUHx7PuLIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_jaccard=[]\n",
        "\n",
        "for ind,row in train.iterrows():\n",
        "    sentence1 = row.text\n",
        "    sentence2 = row.selected_text\n",
        "\n",
        "    jaccard_score = jaccard(sentence1,sentence2)\n",
        "    #print(jaccard_score)\n",
        "    results_jaccard.append([sentence1,sentence2,jaccard_score])\n",
        "print(\"Results of jaccard: \", results_jaccard)    "
      ],
      "metadata": {
        "id": "_cI81XkvuhmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jaccard = pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\n",
        "display(jaccard)\n",
        "train = train.merge(jaccard,how='outer')\n",
        "display(train)\n"
      ],
      "metadata": {
        "id": "s9eyDfHpvxTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['Num_words_ST'] = train['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\n",
        "train['Num_word_text'] = train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\n",
        "train['difference_in_words'] = train['Num_word_text'] - train['Num_words_ST'] #Difference in Number of words text and Selected Text"
      ],
      "metadata": {
        "id": "Vk7DVsEjwWXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(100)"
      ],
      "metadata": {
        "id": "DTvfs04pwWZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the distribution of Meta-Features"
      ],
      "metadata": {
        "id": "s25z3IWtyTe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist_data = [train['Num_words_ST'],train['Num_word_text']]\n",
        "\n",
        "group_labels = ['Selected_Text', 'Text']\n",
        "\n",
        "# Create distplot with custom bin_size\n",
        "fig = ff.create_distplot(hist_data, group_labels,show_curve=False)\n",
        "fig.update_layout(title_text='Distribution of Number Of words')\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    width=900,\n",
        "    height=700,\n",
        "    paper_bgcolor=\"LightSteelBlue\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "7PkjF3pFyUXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "p1=sns.kdeplot(train['Num_words_ST'], shade=True, color=\"r\").set_title('Kernel Distribution of Number Of words')\n",
        "p1=sns.kdeplot(train['Num_word_text'], shade=True, color=\"b\")"
      ],
      "metadata": {
        "id": "46Ir29Njy69j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "p1=sns.kdeplot(train[train['sentiment']=='positive']['difference_in_words'], shade=True, color=\"b\").set_title('Kernel Distribution of Difference in Number Of words')\n",
        "p2=sns.kdeplot(train[train['sentiment']=='negative']['difference_in_words'], shade=True, color=\"r\")"
      ],
      "metadata": {
        "id": "kJfWnCwnwWbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.distplot(train[train['sentiment']=='neutral']['difference_in_words'],kde=False)"
      ],
      "metadata": {
        "id": "AtacONNXwWdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "p1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard_score'], shade=True, color=\"b\").set_title('KDE of Jaccard Scores across different Sentiments')\n",
        "p2=sns.kdeplot(train[train['sentiment']=='negative']['jaccard_score'], shade=True, color=\"r\")\n",
        "#p2=sns.kdeplot(train[train['sentiment']=='neutral']['jaccard_score'], shade=True, color=\"g\")\n",
        "plt.legend(labels=['positive','negative'])"
      ],
      "metadata": {
        "id": "bhzJ8XbzwWf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = train[train['Num_word_text']<=2]"
      ],
      "metadata": {
        "id": "zt770KPkwWiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.groupby('sentiment').mean()['jaccard_score']"
      ],
      "metadata": {
        "id": "1bECUSEewWk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k[k['sentiment']=='positive']"
      ],
      "metadata": {
        "id": "qzA0MB8Ivtt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the Corpus"
      ],
      "metadata": {
        "id": "1nfzYrRUykyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "uA0AFmojvtv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'] = train['text'].apply(lambda x:clean_text(x))\n",
        "train['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))"
      ],
      "metadata": {
        "id": "v8ty5EW3vt2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "MNSWAN4evt6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Common words in our Target-Selected Text"
      ],
      "metadata": {
        "id": "f3N8nzn4zEUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\n",
        "top = Counter([item for sublist in train['temp_list'] for item in sublist])\n",
        "temp = pd.DataFrame(top.most_common(20))\n",
        "temp.columns = ['Common_words','count']\n",
        "temp.style.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "id": "qhClxMJ0vt-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n",
        "             width=700, height=700,color='Common_words')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "iq_ylmMzvuAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')\n",
        "def remove_stopword(x):\n",
        "    return [y for y in x if y not in stopwords.words('english')]\n",
        "train['temp_list'] = train['temp_list'].apply(lambda x:remove_stopword(x))"
      ],
      "metadata": {
        "id": "6GxF_0_jvuDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top = Counter([item for sublist in train['temp_list'] for item in sublist])\n",
        "temp = pd.DataFrame(top.most_common(20))\n",
        "temp = temp.iloc[1:,:]\n",
        "temp.columns = ['Common_words','count']\n",
        "temp.style.background_gradient(cmap='Purples')"
      ],
      "metadata": {
        "id": "O4ylVkovvuF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "_VxcecEE1JOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most common word in text"
      ],
      "metadata": {
        "id": "VNzE7pkI1mNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['temp_list1'] = train['text'].apply(lambda x:str(x).split()) #List of words in every row for text\n",
        "train['temp_list1'] = train['temp_list1'].apply(lambda x:remove_stopword(x)) #Removing Stopwords\n",
        "display(train)"
      ],
      "metadata": {
        "id": "owWSlFj41JQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top = Counter([item for sublist in train['temp_list1'] for item in sublist])\n",
        "temp = pd.DataFrame(top.most_common(25))\n",
        "temp = temp.iloc[1:,:]\n",
        "temp.columns = ['Common_words','count']\n",
        "temp.style.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "id": "ULMeUV5Y1JTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n",
        "             width=700, height=700,color='Common_words')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "mQpG0hWA1JVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most common words Sentiments Wise"
      ],
      "metadata": {
        "id": "jZSAq7-f64GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Positive_sent = train[train['sentiment']=='positive']\n",
        "Negative_sent = train[train['sentiment']=='negative']\n",
        "Neutral_sent = train[train['sentiment']=='neutral']"
      ],
      "metadata": {
        "id": "Ym6DIVep1JXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MosT common positive words\n",
        "top = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\n",
        "# print(top)\n",
        "temp_positive = pd.DataFrame(top.most_common(10))\n",
        "temp_positive.columns = ['Common_positive_words','count']\n",
        "temp_positive.style.background_gradient(cmap='Greens')"
      ],
      "metadata": {
        "id": "7cNFqIPN1JZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MosT common Negative_sent words\n",
        "top = Counter([item for sublist in Negative_sent['temp_list'] for item in sublist])\n",
        "print(len(top))\n",
        "temp_negative = pd.DataFrame(top.most_common(10))\n",
        "temp_negative.columns = ['Common_negative_words','count']\n",
        "temp_negative.style.background_gradient(cmap='Reds')"
      ],
      "metadata": {
        "id": "Ar5sbXNs26Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MosT common Neutral_sent words\n",
        "top = Counter([item for sublist in Neutral_sent['temp_list'] for item in sublist])\n",
        "# print(len(top))\n",
        "temp_neutral = pd.DataFrame(top.most_common(10))\n",
        "temp_neutral.columns = ['Common_Neutral_words','count']\n",
        "temp_neutral.style.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "id": "x-ToC7Cn4DBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.treemap(temp_neutral, path=['Common_Neutral_words'], values='count',title='Tree Of Common_Neutral_words')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "3sVU6V7d4UfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# item = 0\n",
        "# for sublist in Positive_sent['temp_list']:\n",
        "#   print(\"sublist: \",sublist)\n",
        "#   for item in sublist:\n",
        "#     print('item: ',item)\n",
        "#     top = Counter([item])\n",
        "#     print(\"top: \",top)\n",
        "               \n",
        "# # print(item)              \n",
        "               \n",
        "               \n",
        "# # for item in sublist])\n",
        "# positive = 'positive'\n",
        "# allother = []\n",
        "# for item in train[train.positive != positive]['temp_list1']:\n",
        "#   print(\"item\",item)\n",
        "#   for word in item:\n",
        "#       allother .append(word)\n",
        "# allother  = list(set(allother ))"
      ],
      "metadata": {
        "id": "hKIHbIcY1Jbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's Look at Unique Words in each Segment"
      ],
      "metadata": {
        "id": "oNvxaPPI6nJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = [word for word_list in train['temp_list1'] for word in word_list]"
      ],
      "metadata": {
        "id": "kqXqWi8KvuIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def words_unique(sentiment,numwords,raw_words):\n",
        "    '''\n",
        "    Input:\n",
        "        segment - Segment category (ex. 'Neutral');\n",
        "        numwords - how many specific words do you want to see in the final result; \n",
        "        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n",
        "    Output: \n",
        "        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n",
        "\n",
        "    '''\n",
        "    allother = []\n",
        "    for item in train[train.sentiment != sentiment]['temp_list1']:\n",
        "      # print(\"item\",item)\n",
        "      # break\n",
        "      for word in item:\n",
        "          allother .append(word)\n",
        "    allother  = list(set(allother ))\n",
        "    # print('allother: ',allother)\n",
        "    # print(len(allother))\n",
        "    \n",
        "    specificationally = [x for x in raw_text if x not in allother]\n",
        "    # print(\"specificnonly\",specificationally)\n",
        "    # print(len(specificationally))\n",
        "    \n",
        "    mycounter = Counter()\n",
        "    \n",
        "    for item in train[train.sentiment == sentiment]['temp_list1']:\n",
        "        for word in item:\n",
        "            mycounter[word] += 1\n",
        "    keep = list(specificationally)\n",
        "    \n",
        "    for word in list(mycounter):\n",
        "        if word not in keep:\n",
        "            del mycounter[word]\n",
        "    \n",
        "    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n",
        "    \n",
        "    return Unique_words"
      ],
      "metadata": {
        "id": "7nln22Ga7bMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Unique_Positive= words_unique('positive', 20, raw_text)\n",
        "print(\"The top 20 unique words in Positive Tweets are:\")\n",
        "Unique_Positive.style.background_gradient(cmap='Greens')"
      ],
      "metadata": {
        "id": "6jrn7fd57bOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from palettable.colorbrewer.qualitative import Pastel1_7\n",
        "plt.figure(figsize=(16,10))\n",
        "my_circle=plt.Circle((0,0), 0.7, color='white')\n",
        "plt.pie(Unique_Positive['count'], labels=Unique_Positive.words, colors=Pastel1_7.hex_colors)\n",
        "p=plt.gcf()\n",
        "p.gca().add_artist(my_circle)\n",
        "plt.title('DoNut Plot Of Unique Positive Words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eV0-_SkH7bRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Unique_Negative= words_unique('negative', 10, raw_text)\n",
        "print(\"The top 10 unique words in Negative Tweets are:\")\n",
        "Unique_Negative.style.background_gradient(cmap='Reds')"
      ],
      "metadata": {
        "id": "6lgXoN8C7bVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Unique_Neutral= words_unique('neutral', 10, raw_text)\n",
        "print(\"The top 10 unique words in Neutral Tweets are:\")\n",
        "Unique_Neutral.style.background_gradient(cmap='Oranges')"
      ],
      "metadata": {
        "id": "dIdEAUQT7bYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from palettable.colorbrewer.qualitative import Pastel1_7\n",
        "plt.figure(figsize=(16,10))\n",
        "my_circle=plt.Circle((0,0), 0.7, color='white')\n",
        "plt.pie(Unique_Neutral['count'], labels=Unique_Neutral.words, colors=Pastel1_7.hex_colors)\n",
        "p=plt.gcf()\n",
        "p.gca().add_artist(my_circle)\n",
        "plt.title('DoNut Plot Of Unique Neutral Words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JxJGFT1y7bab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's Time For WordClouds"
      ],
      "metadata": {
        "id": "cD5DnWm3M2Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n",
        "                   title = None, title_size=40, image_color=False):\n",
        "    stopwords = set(STOPWORDS)\n",
        "    more_stopwords = {'u', \"im\"}\n",
        "    stopwords = stopwords.union(more_stopwords)\n",
        "\n",
        "    wordcloud = WordCloud(background_color=color,\n",
        "                    stopwords = stopwords,\n",
        "                    max_words = max_words,\n",
        "                    max_font_size = max_font_size, \n",
        "                    random_state = 42,\n",
        "                    width=400, \n",
        "                    height=200,\n",
        "                    mask = mask)\n",
        "    wordcloud.generate(str(text))\n",
        "    \n",
        "    plt.figure(figsize=figure_size)\n",
        "    if image_color:\n",
        "        image_colors = ImageColorGenerator(mask);\n",
        "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n",
        "        plt.title(title, fontdict={'size': title_size,  \n",
        "                                  'verticalalignment': 'bottom'})\n",
        "    else:\n",
        "        plt.imshow(wordcloud);\n",
        "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
        "                                  'verticalalignment': 'bottom'})\n",
        "    plt.axis('off');\n",
        "    plt.tight_layout() \n",
        "d = \"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/\"    "
      ],
      "metadata": {
        "id": "AlVbhae_MmaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_mask = np.array(Image.open(d+ '0x0.jpg'))\n",
        "plot_wordcloud(Neutral_sent.text,mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Neutral Tweets\")"
      ],
      "metadata": {
        "id": "Hu7ZDx7iMmcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(Positive_sent.text,mask=pos_mask,title=\"Word Cloud Of Positive tweets\",title_size=30)"
      ],
      "metadata": {
        "id": "HPvn1f1cMmfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(Negative_sent.text,mask=pos_mask,title=\"Word Cloud of Negative Tweets\",color='white',title_size=30)"
      ],
      "metadata": {
        "id": "deirug8_Mmhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelling"
      ],
      "metadata": {
        "id": "0VyrVcMeXrzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Modelling the Problem as NER"
      ],
      "metadata": {
        "id": "ZVRQMT5_XyfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/train.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/test.csv\")\n",
        "df_submission = pd.read_csv(\"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/submission.csv\")"
      ],
      "metadata": {
        "id": "rZLQdZIbUt3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['Num_words_text'] = df_train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main Text in train set"
      ],
      "metadata": {
        "id": "uX7JJ2ewUt6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[df_train['Num_words_text']>=3]"
      ],
      "metadata": {
        "id": "SO4Oa7gOUt8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(output_dir, nlp, new_model_name):\n",
        "    ''' This Function Saves model to \n",
        "    given output directory'''\n",
        "    \n",
        "    output_dir = f\"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/{output_dir}\"\n",
        "    if output_dir is not None:        \n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        nlp.meta[\"name\"] = new_model_name\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)"
      ],
      "metadata": {
        "id": "i2K0D1PwUt_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass model = nlp if you want to train on top of existing model \n",
        "from spacy.training.example import Example\n",
        "\n",
        "def train(train_data, output_dir, n_iter, model=None):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\n",
        "    remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n",
        "    \"\"\"\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(output_dir)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        # print(\"nlp\",nlp)\n",
        "        print(\"Created blank 'en' model\")\n",
        "    \n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        # ner = nlp.create_pipe(\"ner\")\n",
        "        ner = nlp.add_pipe('ner', last=True)\n",
        "        # # nlp.add_pipe('ner')\n",
        "        # ner = nlp.add_pipe('ner')\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "    \n",
        "    # add labels\n",
        "    for _, annotations in train_data:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "            # print(\"ent\", ent)\n",
        "    # print(\"this is done\")\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        # sizes = compounding(1.0, 4.0, 1.001)\n",
        "        # batch up the examples using spaCy's minibatch\n",
        "        if model is None:\n",
        "            nlp.begin_training()\n",
        "        else:\n",
        "            nlp.resume_training()\n",
        "        # print(\"this is done 2\")\n",
        "\n",
        "        # for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
        "        #     for text, annotations in batch:\n",
        "        #       # create Example\n",
        "        #       doc = nlp.make_doc(text)\n",
        "        #       example = Example.from_dict(doc, annotations)\n",
        "        #       # Update the model\n",
        "        #       nlp.update([example], losses=losses, drop=0.3)\n",
        "\n",
        "\n",
        "        for itn in tqdm(range(n_iter)):\n",
        "          # print(\"this is done 3\")\n",
        "          random.shuffle(train_data)\n",
        "          batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n",
        "          losses = {}\n",
        "          # print(\"this is done 4\")\n",
        "\n",
        "          # for text, annotations in batches:\n",
        "          #   # create Example\n",
        "          #   doc = nlp.make_doc(text)\n",
        "          #   example = Example.from_dict(doc, annotations)\n",
        "          #   # Update the model\n",
        "          #   nlp.update([example], losses=losses, drop=0.3)\n",
        "          # for batch in batches:\n",
        "          #   print(\"this is done 5\")\n",
        "          #   texts, annotations = zip(*batch)\n",
        "          #   nlp.update(texts,  # batch of texts\n",
        "          #               annotations,  # batch of annotations\n",
        "          #               drop=0.5,   # dropout - make it harder to memorise data\n",
        "          #               losses=losses, \n",
        "          #               )\n",
        "\n",
        "          for batch in batches:\n",
        "            texts, annotations = zip(*batch)\n",
        "            # print(\"texts\",texts)\n",
        "            \n",
        "            example = []\n",
        "            # Update the model with iterating each text\n",
        "            for i in range(len(texts)):\n",
        "                doc = nlp.make_doc(texts[i])\n",
        "                # print(\"doc\",doc)\n",
        "                example.append(Example.from_dict(doc, annotations[i]))\n",
        "                # print(\"example\",example)\n",
        "            \n",
        "            # Update the model\n",
        "            nlp.update(example, drop=0.5, losses=losses)\n",
        "\n",
        "        print(\"Losses\", losses)\n",
        "    save_model(output_dir, nlp, 'st_ner')"
      ],
      "metadata": {
        "id": "pAHy0Bv2Uue5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_out_path(sentiment):\n",
        "    '''\n",
        "    Returns Model output path\n",
        "    '''\n",
        "    model_out_path = None\n",
        "    if sentiment == 'positive':\n",
        "        model_out_path = \"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/archive/models/model_pos\"\n",
        "    elif sentiment == 'negative':\n",
        "        model_out_path = \"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/archive/models/model_neg\"\n",
        "    print(\"get_model_out_path is done\")\n",
        "    return model_out_path\n"
      ],
      "metadata": {
        "id": "1ff1d8voUuiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_data(sentiment):\n",
        "    '''\n",
        "    Returns Trainong data in the format needed to train spacy NER\n",
        "    '''\n",
        "    train_data = []\n",
        "    for index, row in df_train.iterrows():\n",
        "        if row.sentiment == sentiment:\n",
        "            selected_text = row.selected_text\n",
        "            text = row.text\n",
        "            start = text.find(selected_text)\n",
        "            end = start + len(selected_text)\n",
        "            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n",
        "    print(\"get_training_data data is done\")        \n",
        "    return train_data"
      ],
      "metadata": {
        "id": "VwiI8SfKMmkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = 'positive'\n",
        "train_data = get_training_data(sentiment)\n",
        "model_path = get_model_out_path(sentiment)\n",
        "# For DEmo Purposes I have taken 3 iterations you can train the model as you want\n",
        "train(train_data, model_path, n_iter=3, model=None)"
      ],
      "metadata": {
        "id": "KB8w-XGBMmrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = 'negative'\n",
        "\n",
        "train_data = get_training_data(sentiment)\n",
        "model_path = get_model_out_path(sentiment)\n",
        "\n",
        "train(train_data, model_path, n_iter=3, model=None)"
      ],
      "metadata": {
        "id": "abLZMyGYf-Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting with the trained Model"
      ],
      "metadata": {
        "id": "69KJ-dIGwxyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_entities(text, model):\n",
        "    doc = model(text)\n",
        "    ent_array = []\n",
        "    for ent in doc.ents:\n",
        "        start = text.find(ent.text)\n",
        "        end = start + len(ent.text)\n",
        "        new_int = [start, end, ent.label_]\n",
        "        if new_int not in ent_array:\n",
        "            ent_array.append([start, end, ent.label_])\n",
        "    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n",
        "    return selected_text"
      ],
      "metadata": {
        "id": "beURuTngf-Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_texts = []\n",
        "MODELS_BASE_PATH = \"/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/content/drive/MyDrive/Master's_thesis/tweet_sentiment_analysis/sentiment_analysis/archive/models/\"\n",
        "\n",
        "if MODELS_BASE_PATH is not None:\n",
        "    print(\"Loading Models  from \", MODELS_BASE_PATH)\n",
        "    model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n",
        "    model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n",
        "        \n",
        "    for index, row in df_test.iterrows():\n",
        "        text = row.text\n",
        "        output_str = \"\"\n",
        "        if row.sentiment == 'neutral' or len(text.split()) <= 2:\n",
        "            selected_texts.append(text)\n",
        "        elif row.sentiment == 'positive':\n",
        "            selected_texts.append(predict_entities(text, model_pos))\n",
        "        else:\n",
        "            selected_texts.append(predict_entities(text, model_neg))\n",
        "        \n",
        "df_test['selected_text'] = selected_texts\n"
      ],
      "metadata": {
        "id": "sogUsEPif-YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission['selected_text'] = df_test['selected_text']\n",
        "df_submission.to_csv(\"submission.csv\", index=False)\n",
        "display(df_submission.head(10))"
      ],
      "metadata": {
        "id": "JnsaA2FXf-bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CGzWvOMef-dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DxoZenRxf-mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AeP-WLWvf-qD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}